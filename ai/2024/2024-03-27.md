# SPDX AI Team Meeting 2024-03-27

## Attendees
- Karen Bennet
- Kate Stewart
- Mark Atwood (Amazon.com, Inc)
- Victor Lu
- Amaya Naik (interested in LLM)
- Helen Oakley
- Gopi Krishnan Rajbahadur
- Mihai Maruseac
- Stefana Simion
- Sean Morgan
- Andreas Fehlner
- Dmitry Raidman

## Agenda
- Welcome new members
- Approve previous meeting minutes
  - 2024-03-20 https://github.com/spdx/meetings/pull/655
  - 2024-03-13 https://github.com/spdx/meetings/pull/654
- Review outstanding model issues and pull requests
- [3.1] Background considerations (Karen)
- [3.1] What we need for LLMs? (Gopi)

### PR/Issues
- 671 (formerly 658) [3.0] Dataset: Rename Dataset class to DatasetPackage and update properties desc.
  https://github.com/spdx/spdx-3-model/pull/671
- 677 [3.0] AI: Rename: energyConsumption to trainingEnergyConsumption
  https://github.com/spdx/spdx-3-model/issues/677

### Discussion
- Mark recommends that we write up a spec for each regulation, rather than trying to parse into fields.
  We are seeing different naming, fields, etc. 
- Karen pointed to what needed to be in an AI BOM for the FDA - and there was a useful mapping.
  Karen thinking of writing similar with EU AI Act.
- Mark - remember that from regulator perspective, it needs to be very structured mapping, with fields.
  Lawmakers - their laws are the center of the world, so use their terms.
- AIBOM workshop at RSA: https://lu.ma/AIBOM-workshop-RSAC2024
- Kate, Karen & Helen will work on message for AI BOM Workshop.
- Karen looking at Risk Cards https://arxiv.org/abs/2303.18190 - that are starting to propigate.
  Green Foundation, with environment type fields. IEEE working on "Planet Positive" initiative.

- LLM
  - How should we indicate adaption/evolution of models over time. Bringing in learning from new data.
  - Benchmark and testing criteria - exploring other algorithms, and change biases in reasoning. 
  - https://arxiv.org/abs/2307.09009 - How would you know the weights donâ€™t change behind an API?
    The weights can change in a lot of models based on usage,
    if the model is behind an API you can never know if it uses online learning.
  - LLMs farms out to many AI systems; dynamics of systems of systems.
  - Bias field may need to become more nuanced? Overrepresenting certain types of information.
  - LLM output - metric field. Once we deploy and AI system with a LLM component in it,
    it can digress over time. "Runtime BOM" vs "Deployed BOM"?
  - Use of datasets like "the pile" - slices may have specific problems
    (too biased, copyright information, etc.)
  - Add validated on relations - Gopi
  - Add fine tuned on as a possible relationship; as well as possible composition different than "depends on"?
  - The weights can change in a lot of models based on usage,
    if the model is behind an API you can never know if it uses online learning.
  - Information about hardware in 3.1, may move some of our fields into there?
  - Environmental profile? possibly part of AI or System?
  - Karen looking at RISK Cards https://arxiv.org/abs/2303.18190 and thinks it is worth tracking.
    (CMU & U of Washington paper)
  - Hardware configurations for LLMs is a bit of a nightmare.

- VEX Presentation from Dimitry - showing the statuses.
  Hardware will play a large role as well.
  What does Vulnerability mean for a system?
  What does it mean for capturing risks and biases?
- When model & data; model & hardware; are all vectors for issues.
  CVE number - what are the characteristics of how the AI system is built.
  How are variables being trapped in SBOM and used.
- Models: code could have tampering which create unattended consequences.
- More data oriented than traditional systems.

## Notes
- Walk through a systems of systems in upcoming meetings - to check we have what we need.
- Gopi's AI system architecture
  - Rethinking Software Engineering in the Foundation Model Era:
    A Curated Catalogue of Challenges in the Development of Trustworthy FMware
    https://arxiv.org/abs/2402.15943
  - Has reference to SPDX; nomenclature of system engineering proposal.
- White paper about Model Openness Framework (MOF) overlap with AIBOM
  - The Model Openness Framework: Promoting Completeness and Openness for
    Reproducibility, Transparency and Usability in AI
    https://arxiv.org/abs/2403.13784
- NOTE: we need to keep all the fields, but can reuse them in other profiles.
  Can only deprecate, not delete, until 4.0.
  https://genaicommons.org/resources/publications/ is where there is information about Model Openness Framework (MOF).
- Karen to point RISK Card. Consider VEX as a place for RISK?
- Dmitry pointed to cve: Is there something equivalent to this for AIVULNS?
  https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?name=CVE-2023-5072;
  there are modifiers? no score at this point.
  Not always easy to identify if it's a vulnerability yet - could be weakness or risk.
  May not have vuln, but there are risks we may want to surface.
- Dmitry flagged https://lu.ma/rsa-sbom-meetup - SBOM meetup happening at RSA - if interested, please sign up
